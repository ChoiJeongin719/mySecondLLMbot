import os
import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv
import json
import glob

# Load environment variables from .env file
load_dotenv()

# Set page config
st.set_page_config(
    page_title="í† ë¡  ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Initialize session state variables if they don't exist
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! í† ë¡  ì±„íŒ…ë°©ì…ë‹ˆë‹¤. í† ë¡ í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "type": "system"}]

if "system_message_pro" not in st.session_state:
    st.session_state.system_message_pro = "ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì œì‹œí•œ ì£¼ì œì— ëŒ€í•´ ì°¬ì„± ì…ì¥ì„ ì·¨í•˜ëŠ” í† ë¡ ìì…ë‹ˆë‹¤. ë…¼ë¦¬ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ì°¬ì„± ì…ì¥ì˜ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”."

if "system_message_con" not in st.session_state:
    st.session_state.system_message_con = "ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì œì‹œí•œ ì£¼ì œì— ëŒ€í•´ ë°˜ëŒ€ ì…ì¥ì„ ì·¨í•˜ëŠ” í† ë¡ ìì…ë‹ˆë‹¤. ë…¼ë¦¬ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ë°˜ëŒ€ ì…ì¥ì˜ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”."

if "usage_stats" not in st.session_state:
    st.session_state.usage_stats = []

if "show_process" not in st.session_state:
    st.session_state.show_process = False

def get_openai_client():
    """Create and return an OpenAI client configured with environment variables"""
    token = os.getenv("GITHUB_TOKEN")
    endpoint = os.getenv("GITHUB_ENDPOINT", "https://models.github.ai/inference")
    
    if not token:
        st.error("GitHub token not found in environment variables. Please check your .env file.")
        st.stop()
        
    return OpenAI(
        base_url=endpoint,
        api_key=token,
    )

def generate_debate_responses(prompt):
    """Generate two separate responses - one pro, one con"""
    client = get_openai_client()
    model_name = os.getenv("GITHUB_MODEL", "openai/gpt-4o")
    
    # Prepare previous conversation history (excluding the system message)
    history = []
    for msg in st.session_state.messages:
        if msg["role"] != "system" and "type" not in msg:  # Skip system messages
            history.append(msg)
    
    # ì°¬ì„± ë©”ì‹œì§€ ìƒì„±
    pro_messages = [{"role": "system", "content": st.session_state.system_message_pro}] + history + [{"role": "user", "content": prompt}]
    
    # ë°˜ëŒ€ ë©”ì‹œì§€ ìƒì„±
    con_messages = [{"role": "system", "content": st.session_state.system_message_con}] + history + [{"role": "user", "content": prompt}]
    
    try:
        # ì°¬ì„± ì‘ë‹µ ì»¨í…Œì´ë„ˆ ë° í”Œë ˆì´ìŠ¤í™€ë” ì„¤ì •
        pro_container = st.container()
        pro_placeholder = pro_container.empty()
        
        # ë°˜ëŒ€ ì‘ë‹µ ì»¨í…Œì´ë„ˆ ë° í”Œë ˆì´ìŠ¤í™€ë” ì„¤ì •
        con_container = st.container()
        con_placeholder = con_container.empty()
        
        # ì°¬ì„± ì‘ë‹µ ìƒì„±
        full_pro_response = ""
        usage_pro = None
        
        pro_response = client.chat.completions.create(
            messages=pro_messages,
            model=model_name,
            stream=True,
            stream_options={'include_usage': True}
        )
        
        # ì°¬ì„± ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        for chunk in pro_response:
            if chunk.choices and chunk.choices[0].delta.content:
                content_chunk = chunk.choices[0].delta.content
                full_pro_response += content_chunk
                pro_placeholder.markdown(
                    f"<div style='background-color: #90EE90; padding: 10px; border-radius: 5px;'>{full_pro_response}â–Œ</div>",
                    unsafe_allow_html=True
                )
                    
            if chunk.usage:
                usage_pro = chunk.usage
        
        # ìµœì¢… ì°¬ì„± ì‘ë‹µ ì—…ë°ì´íŠ¸ (ì»¤ì„œ ì œê±°)
        pro_placeholder.markdown(
            f"<div style='background-color: #90EE90; padding: 10px; border-radius: 5px;'>{full_pro_response}</div>",
            unsafe_allow_html=True
        )
        
        # ë°˜ëŒ€ ì‘ë‹µ ìƒì„±
        full_con_response = ""
        usage_con = None
        
        con_response = client.chat.completions.create(
            messages=con_messages,
            model=model_name,
            stream=True,
            stream_options={'include_usage': True}
        )
        
        # ë°˜ëŒ€ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        for chunk in con_response:
            if chunk.choices and chunk.choices[0].delta.content:
                content_chunk = chunk.choices[0].delta.content
                full_con_response += content_chunk
                con_placeholder.markdown(
                    f"<div style='background-color: #FFB6C1; padding: 10px; border-radius: 5px;'>{full_con_response}â–Œ</div>",
                    unsafe_allow_html=True
                )
                    
            if chunk.usage:
                usage_con = chunk.usage
        
        # ìµœì¢… ë°˜ëŒ€ ì‘ë‹µ ì—…ë°ì´íŠ¸ (ì»¤ì„œ ì œê±°)
        con_placeholder.markdown(
            f"<div style='background-color: #FFB6C1; padding: 10px; border-radius: 5px;'>{full_con_response}</div>",
            unsafe_allow_html=True
        )
        
        # ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": full_pro_response, "type": "pro"})
        st.session_state.messages.append({"role": "assistant", "content": full_con_response, "type": "con"})
        
        # ì‚¬ìš©ëŸ‰ í†µê³„ ì €ì¥
        if usage_pro and usage_con:
            # Pydantic ì²˜ë¦¬
            usage_pro_dict = usage_pro.model_dump() if hasattr(usage_pro, 'model_dump') else usage_pro.dict()
            usage_con_dict = usage_con.model_dump() if hasattr(usage_con, 'model_dump') else usage_con.dict()
            
            st.session_state.usage_stats.append({
                "prompt_tokens": usage_pro_dict.get("prompt_tokens", 0) + usage_con_dict.get("prompt_tokens", 0),
                "completion_tokens": usage_pro_dict.get("completion_tokens", 0) + usage_con_dict.get("completion_tokens", 0),
                "total_tokens": usage_pro_dict.get("total_tokens", 0) + usage_con_dict.get("total_tokens", 0)
            })
        
        # í”„ë¡œì„¸ìŠ¤ í‘œì‹œ í™œì„±í™”ëœ ê²½ìš°
        if st.session_state.show_process:
            process_container = st.container()
            with process_container:
                st.markdown("### ëª¨ë¸ ì²˜ë¦¬ ê³¼ì •")
                
                # ìš”ì²­ ìƒì„¸ ì •ë³´ í‘œì‹œ
                request_expander = st.expander("ìš”ì²­ ìƒì„¸ ì •ë³´", expanded=False)
                with request_expander:
                    st.markdown("**ì°¬ì„± ì‹œìŠ¤í…œ ë©”ì‹œì§€:**")
                    st.code(st.session_state.system_message_pro)
                    st.markdown("**ë°˜ëŒ€ ì‹œìŠ¤í…œ ë©”ì‹œì§€:**")
                    st.code(st.session_state.system_message_con)
                    st.markdown("**ì‚¬ìš©ì ì…ë ¥:**")
                    st.code(prompt)
                
                # ì›ì‹œ ì‘ë‹µ í‘œì‹œ
                response_expander = st.expander("ì›ì‹œ ì‘ë‹µ", expanded=False)
                with response_expander:
                    st.markdown("**ì°¬ì„± ì‘ë‹µ:**")
                    st.code(full_pro_response, language="markdown")
                    st.markdown("**ë°˜ëŒ€ ì‘ë‹µ:**")
                    st.code(full_con_response, language="markdown")
                
                # ì‚¬ìš©ëŸ‰ í†µê³„ í‘œì‹œ
                if usage_pro and usage_con:
                    usage_expander = st.expander("ì‚¬ìš©ëŸ‰ í†µê³„", expanded=False)
                    with usage_expander:
                        st.markdown("**ì°¬ì„± ì‘ë‹µ ì‚¬ìš©ëŸ‰:**")
                        st.markdown(f"- í”„ë¡¬í”„íŠ¸ í† í°: {usage_pro_dict.get('prompt_tokens', 0)}")
                        st.markdown(f"- ì‘ë‹µ í† í°: {usage_pro_dict.get('completion_tokens', 0)}")
                        st.markdown(f"- ì´ í† í°: {usage_pro_dict.get('total_tokens', 0)}")
                        
                        st.markdown("**ë°˜ëŒ€ ì‘ë‹µ ì‚¬ìš©ëŸ‰:**")
                        st.markdown(f"- í”„ë¡¬í”„íŠ¸ í† í°: {usage_con_dict.get('prompt_tokens', 0)}")
                        st.markdown(f"- ì‘ë‹µ í† í°: {usage_con_dict.get('completion_tokens', 0)}")
                        st.markdown(f"- ì´ í† í°: {usage_con_dict.get('total_tokens', 0)}")
        
        return True
    except Exception as e:
        st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

# CSS ìŠ¤íƒ€ì¼ ì •ì˜ ì¶”ê°€
st.markdown("""
    <style>
    .pro-bubble {
        background-color: #90EE90;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 10px;
    }
    .con-bubble {
        background-color: #FFB6C1;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 10px;
    }
    .user-bubble {
        background-color: #F0F2F6;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 10px;
        text-align: right;
    }
    .system-bubble {
        background-color: #E8E8E8;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 10px;
    }
    .stChatFloatingInputContainer {
        position: fixed !important;
        bottom: 0 !important;
        padding: 1rem !important;
        width: calc(100% - 250px) !important; /* ì‚¬ì´ë“œë°” ë„ˆë¹„ ì¡°ì • */
        background-color: white !important;
        z-index: 1000 !important;
    }
    .main-content {
        padding-bottom: 100px; /* ê³ ì • ì…ë ¥ì°½ì„ ìœ„í•œ í•˜ë‹¨ ì—¬ë°± */
    }
    </style>
""", unsafe_allow_html=True)

# UI Layout
st.title("ğŸ¤– í† ë¡  ì±—ë´‡")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.subheader("ì„¤ì •")
    
    # ì°¬ì„± ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìˆ˜ì •
    st.text_area(
        "ì°¬ì„± ì±—ë´‡ ì„¤ì •", 
        value=st.session_state.system_message_pro,
        key="system_message_pro_input",
        height=150
    )
    
    # ë°˜ëŒ€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìˆ˜ì •
    st.text_area(
        "ë°˜ëŒ€ ì±—ë´‡ ì„¤ì •", 
        value=st.session_state.system_message_con,
        key="system_message_con_input",
        height=150
    )
    
    if st.button("ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸"):
        st.session_state.system_message_pro = st.session_state.system_message_pro_input
        st.session_state.system_message_con = st.session_state.system_message_con_input
        st.success("ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ê¸°íƒ€ ì‚¬ì´ë“œë°” ìš”ì†Œ
    st.markdown("---")
    
    # ì±„íŒ… ê¸°ë¡ ë³´ê¸°
    with st.expander("ì±„íŒ… ê¸°ë¡ ë³´ê¸°"):
        st.json(st.session_state.messages)
    
    # ì‚¬ìš©ëŸ‰ í†µê³„ ë³´ê¸°
    with st.expander("ì‚¬ìš©ëŸ‰ í†µê³„ ë³´ê¸°"):
        if st.session_state.usage_stats:
            for i, usage in enumerate(st.session_state.usage_stats):
                st.write(f"ë©”ì‹œì§€ {i+1}:")
                st.write(f"- í”„ë¡¬í”„íŠ¸ í† í°: {usage['prompt_tokens']}")
                st.write(f"- ì‘ë‹µ í† í°: {usage['completion_tokens']}")
                st.write(f"- ì´ í† í°: {usage['total_tokens']}")
                st.divider()
            
            # ì´ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            total_prompt = sum(u["prompt_tokens"] for u in st.session_state.usage_stats)
            total_completion = sum(u["completion_tokens"] for u in st.session_state.usage_stats)
            total = sum(u["total_tokens"] for u in st.session_state.usage_stats)
            
            st.write("### ì´ ì‚¬ìš©ëŸ‰")
            st.write(f"- ì´ í”„ë¡¬í”„íŠ¸ í† í°: {total_prompt}")
            st.write(f"- ì´ ì‘ë‹µ í† í°: {total_completion}")
            st.write(f"- ì´ í† í°: {total}")
        else:
            st.write("ì•„ì§ ì‚¬ìš©ëŸ‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì±„íŒ… ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ì±„íŒ… ì´ˆê¸°í™”"):
        st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! í† ë¡  ì±„íŒ…ë°©ì…ë‹ˆë‹¤. í† ë¡ í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "type": "system"}]
        st.session_state.usage_stats = []
        st.success("ì±„íŒ… ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # í”„ë¡œì„¸ìŠ¤ í‘œì‹œ í† ê¸€
    st.markdown("---")
    st.session_state.show_process = st.checkbox("ëª¨ë¸ ì²˜ë¦¬ ê³¼ì • ë³´ê¸°", value=st.session_state.show_process)

# ë©”ì¸ ì±„íŒ… ì˜ì—­
chat_container = st.container()
with chat_container:
    st.markdown('<div class="main-content">', unsafe_allow_html=True)
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ - st.chat_message ëŒ€ì‹  ì»¤ìŠ¤í…€ HTML ì‚¬ìš©
    for message in st.session_state.messages:
        if message["role"] == "user":
            # ì‚¬ìš©ì ë©”ì‹œì§€ - ì±—ë´‡ ì•„ì´ì½˜ ì—†ì´ í‘œì‹œ
            st.markdown(
                f"<div class='user-bubble'>{message['content']}</div>",
                unsafe_allow_html=True
            )
        elif message["role"] == "assistant":
            if "type" in message:
                if message["type"] == "pro":
                    # ì°¬ì„± ì˜ê²¬ (ë ˆì´ë¸” ì œê±°)
                    st.markdown(
                        f"<div class='pro-bubble'>{message['content']}</div>",
                        unsafe_allow_html=True
                    )
                elif message["type"] == "con":
                    # ë°˜ëŒ€ ì˜ê²¬ (ë ˆì´ë¸” ì œê±°)
                    st.markdown(
                        f"<div class='con-bubble'>{message['content']}</div>",
                        unsafe_allow_html=True
                    )
                elif message["type"] == "system":
                    # ì‹œìŠ¤í…œ ë©”ì‹œì§€
                    st.markdown(
                        f"<div class='system-bubble'>{message['content']}</div>",
                        unsafe_allow_html=True
                    )
            else:
                # ê¸°ë³¸ ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€
                st.markdown(
                    f"<div class='system-bubble'>{message['content']}</div>",
                    unsafe_allow_html=True
                )
    
    st.markdown('</div>', unsafe_allow_html=True)

# ì±„íŒ… ì…ë ¥
if prompt := st.chat_input("í† ë¡ í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ - ì±—ë´‡ ì•„ì´ì½˜ ì—†ì´ í‘œì‹œ
    st.markdown(
        f"<div class='user-bubble'>{prompt}</div>",
        unsafe_allow_html=True
    )
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê¸°ë¡ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ì°¬ì„±/ë°˜ëŒ€ ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    generate_debate_responses(prompt)
