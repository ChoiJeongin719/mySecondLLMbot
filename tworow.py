import os
import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv
import json
import glob
import threading

# Load environment variables from .env file
load_dotenv()

# Set page config
st.set_page_config(
    page_title="í† ë¡  ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Initialize session state variables if they don't exist
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! í† ë¡  ì±„íŒ…ë°©ì…ë‹ˆë‹¤. í† ë¡ í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "type": "system"}]

if "system_message_pro" not in st.session_state:
    st.session_state.system_message_pro = "ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì œì‹œí•œ ì£¼ì œì— ëŒ€í•´ ì°¬ì„± ì…ì¥ì„ ì·¨í•˜ëŠ” í† ë¡ ìì…ë‹ˆë‹¤. ë…¼ë¦¬ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ì°¬ì„± ì…ì¥ì˜ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”."

if "system_message_con" not in st.session_state:
    st.session_state.system_message_con = "ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì œì‹œí•œ ì£¼ì œì— ëŒ€í•´ ë°˜ëŒ€ ì…ì¥ì„ ì·¨í•˜ëŠ” í† ë¡ ìì…ë‹ˆë‹¤. ë…¼ë¦¬ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ë°˜ëŒ€ ì…ì¥ì˜ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”."

if "usage_stats" not in st.session_state:
    st.session_state.usage_stats = []

if "show_process" not in st.session_state:
    st.session_state.show_process = False

def get_openai_client():
    """Create and return an OpenAI client configured with environment variables"""
    token = os.getenv("GITHUB_TOKEN")
    endpoint = os.getenv("GITHUB_ENDPOINT", "https://models.github.ai/inference")
    
    if not token:
        st.error("GitHub token not found in environment variables. Please check your .env file.")
        st.stop()
        
    return OpenAI(
        base_url=endpoint,
        api_key=token,
    )

def generate_debate_responses(prompt):
    """Generate two separate responses - one pro, one con simultaneously"""
    client = get_openai_client()
    model_name = os.getenv("GITHUB_MODEL", "openai/gpt-4o")
    
    # Prepare previous conversation history (excluding the system message)
    history = []
    for msg in st.session_state.messages:
        if msg["role"] != "system" and "type" not in msg:  # Skip system messages
            history.append(msg)
    
    # ì°¬ì„± ë©”ì‹œì§€ ìƒì„±
    pro_messages = [{"role": "system", "content": st.session_state.system_message_pro}] + history + [{"role": "user", "content": prompt}]
    
    # ë°˜ëŒ€ ë©”ì‹œì§€ ìƒì„±
    con_messages = [{"role": "system", "content": st.session_state.system_message_con}] + history + [{"role": "user", "content": prompt}]
    
    try:
        # 2ì—´ ë ˆì´ì•„ì›ƒ ìƒì„± (ë” ë„“ì€ ê°„ê²©ìœ¼ë¡œ ì¡°ì •)
        cols = st.columns([1, 0.2, 1])  # ì™¼ìª½ ì¹¼ëŸ¼, ê°„ê²©, ì˜¤ë¥¸ìª½ ì¹¼ëŸ¼
        
        # ì°¬ì„± ì‘ë‹µ ì»¨í…Œì´ë„ˆ ë° í”Œë ˆì´ìŠ¤í™€ë” ì„¤ì • (ì™¼ìª½ ì¹¼ëŸ¼)
        with cols[0]:
            st.markdown("<h4 style='text-align: center;'>ì°¬ì„± ì˜ê²¬</h4>", unsafe_allow_html=True)
            pro_placeholder = st.empty()
        
        # ë°˜ëŒ€ ì‘ë‹µ ì»¨í…Œì´ë„ˆ ë° í”Œë ˆì´ìŠ¤í™€ë” ì„¤ì • (ì˜¤ë¥¸ìª½ ì¹¼ëŸ¼)
        with cols[2]:
            st.markdown("<h4 style='text-align: center;'>ë°˜ëŒ€ ì˜ê²¬</h4>", unsafe_allow_html=True)
            con_placeholder = st.empty()
        
        # ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜
        full_pro_response = ""
        full_con_response = ""
        usage_pro = None
        usage_con = None
        
        # ìŠ¤ë ˆë“œ ì™„ë£Œ í”Œë˜ê·¸
        pro_completed = threading.Event()
        con_completed = threading.Event()
        
        # ì°¬ì„± ì‘ë‹µ ìƒì„± í•¨ìˆ˜
        def generate_pro_response():
            nonlocal full_pro_response, usage_pro
            
            pro_response = client.chat.completions.create(
                messages=pro_messages,
                model=model_name,
                stream=True,
                stream_options={'include_usage': True}
            )
            
            # ì°¬ì„± ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
            for chunk in pro_response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content_chunk = chunk.choices[0].delta.content
                    full_pro_response += content_chunk
                    pro_placeholder.markdown(
                        f"<div style='background-color: #90EE90; padding: 10px; border-radius: 5px;'>{full_pro_response}â–Œ</div>",
                        unsafe_allow_html=True
                    )
                        
                if chunk.usage:
                    usage_pro = chunk.usage
            
            # ìµœì¢… ì°¬ì„± ì‘ë‹µ ì—…ë°ì´íŠ¸ (ì»¤ì„œ ì œê±°)
            pro_placeholder.markdown(
                f"<div style='background-color: #90EE90; padding: 10px; border-radius: 5px;'>{full_pro_response}</div>",
                unsafe_allow_html=True
            )
            
            # ì™„ë£Œ í‘œì‹œ
            pro_completed.set()
        
        # ë°˜ëŒ€ ì‘ë‹µ ìƒì„± í•¨ìˆ˜
        def generate_con_response():
            nonlocal full_con_response, usage_con
            
            con_response = client.chat.completions.create(
                messages=con_messages,
                model=model_name,
                stream=True,
                stream_options={'include_usage': True}
            )
            
            # ë°˜ëŒ€ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
            for chunk in con_response:
                if chunk.choices and chunk.choices[0].delta.content:
                    content_chunk = chunk.choices[0].delta.content
                    full_con_response += content_chunk
                    con_placeholder.markdown(
                        f"<div style='background-color: #FFB6C1; padding: 10px; border-radius: 5px;'>{full_con_response}â–Œ</div>",
                        unsafe_allow_html=True
                    )
                        
                if chunk.usage:
                    usage_con = chunk.usage
            
            # ìµœì¢… ë°˜ëŒ€ ì‘ë‹µ ì—…ë°ì´íŠ¸ (ì»¤ì„œ ì œê±°)
            con_placeholder.markdown(
                f"<div style='background-color: #FFB6C1; padding: 10px; border-radius: 5px;'>{full_con_response}</div>",
                unsafe_allow_html=True
            )
            
            # ì™„ë£Œ í‘œì‹œ
            con_completed.set()
        
        # ë‘ ìŠ¤ë ˆë“œ ìƒì„± ë° ì‹œì‘
        pro_thread = threading.Thread(target=generate_pro_response)
        con_thread = threading.Thread(target=generate_con_response)
        
        pro_thread.start()
        con_thread.start()
        
        # ë‘ ìŠ¤ë ˆë“œê°€ ëª¨ë‘ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        pro_completed.wait()
        con_completed.wait()
        
        # ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": full_pro_response, "type": "pro"})
        st.session_state.messages.append({"role": "assistant", "content": full_con_response, "type": "con"})
        
        # ì‚¬ìš©ëŸ‰ í†µê³„ ì €ì¥
        if usage_pro and usage_con:
            # Pydantic ì²˜ë¦¬
            usage_pro_dict = usage_pro.model_dump() if hasattr(usage_pro, 'model_dump') else usage_pro.dict()
            usage_con_dict = usage_con.model_dump() if hasattr(usage_con, 'model_dump') else usage_con.dict()
            
            st.session_state.usage_stats.append({
                "prompt_tokens": usage_pro_dict.get("prompt_tokens", 0) + usage_con_dict.get("prompt_tokens", 0),
                "completion_tokens": usage_pro_dict.get("completion_tokens", 0) + usage_con_dict.get("completion_tokens", 0),
                "total_tokens": usage_pro_dict.get("total_tokens", 0) + usage_con_dict.get("total_tokens", 0)
            })
        
        # í”„ë¡œì„¸ìŠ¤ í‘œì‹œ í™œì„±í™”ëœ ê²½ìš°
        if st.session_state.show_process:
            process_container = st.container()
            with process_container:
                st.markdown("### ëª¨ë¸ ì²˜ë¦¬ ê³¼ì •")
                
                # ìš”ì²­ ìƒì„¸ ì •ë³´ í‘œì‹œ
                request_expander = st.expander("ìš”ì²­ ìƒì„¸ ì •ë³´", expanded=False)
                with request_expander:
                    st.markdown("**ì°¬ì„± ì‹œìŠ¤í…œ ë©”ì‹œì§€:**")
                    st.code(st.session_state.system_message_pro)
                    st.markdown("**ë°˜ëŒ€ ì‹œìŠ¤í…œ ë©”ì‹œì§€:**")
                    st.code(st.session_state.system_message_con)
                    st.markdown("**ì‚¬ìš©ì ì…ë ¥:**")
                    st.code(prompt)
                
                # ì›ì‹œ ì‘ë‹µ í‘œì‹œ
                response_expander = st.expander("ì›ì‹œ ì‘ë‹µ", expanded=False)
                with response_expander:
                    st.markdown("**ì°¬ì„± ì‘ë‹µ:**")
                    st.code(full_pro_response, language="markdown")
                    st.markdown("**ë°˜ëŒ€ ì‘ë‹µ:**")
                    st.code(full_con_response, language="markdown")
                
                # ì‚¬ìš©ëŸ‰ í†µê³„ í‘œì‹œ
                if usage_pro and usage_con:
                    usage_expander = st.expander("ì‚¬ìš©ëŸ‰ í†µê³„", expanded=False)
                    with usage_expander:
                        st.markdown("**ì°¬ì„± ì‘ë‹µ ì‚¬ìš©ëŸ‰:**")
                        st.markdown(f"- í”„ë¡¬í”„íŠ¸ í† í°: {usage_pro_dict.get('prompt_tokens', 0)}")
                        st.markdown(f"- ì‘ë‹µ í† í°: {usage_pro_dict.get('completion_tokens', 0)}")
                        st.markdown(f"- ì´ í† í°: {usage_pro_dict.get('total_tokens', 0)}")
                        
                        st.markdown("**ë°˜ëŒ€ ì‘ë‹µ ì‚¬ìš©ëŸ‰:**")
                        st.markdown(f"- í”„ë¡¬í”„íŠ¸ í† í°: {usage_con_dict.get('prompt_tokens', 0)}")
                        st.markdown(f"- ì‘ë‹µ í† í°: {usage_con_dict.get('completion_tokens', 0)}")
                        st.markdown(f"- ì´ í† í°: {usage_con_dict.get('total_tokens', 0)}")
        
        return True
    except Exception as e:
        st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

# CSS ìŠ¤íƒ€ì¼ ì •ì˜ ì¶”ê°€
st.markdown("""
    <style>
    .pro-bubble {
        background-color: #90EE90;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 10px;
        max-width: 95%;  /* ë²„ë¸” ë„ˆë¹„ ì œí•œ */
        margin-left: auto;
        margin-right: auto;
    }
    .con-bubble {
        background-color: #FFB6C1;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 10px;
        max-width: 95%;  /* ë²„ë¸” ë„ˆë¹„ ì œí•œ */
        margin-left: auto;
        margin-right: auto;
    }
    .user-bubble {
        background-color: #F0F2F6;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 10px;
        text-align: right;
        max-width: 50%;
        margin-left: auto;
    }
    .system-bubble {
        background-color: #E8E8E8;
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 10px;
        max-width: 50%;
        margin-left: auto;
        margin-right: auto;
    }
    .stChatFloatingInputContainer {
        position: fixed !important;
        bottom: 0 !important;
        padding: 1rem !important;
        width: calc(100% - 250px) !important; /* ì‚¬ì´ë“œë°” ë„ˆë¹„ ì¡°ì • */
        background-color: white !important;
        z-index: 1000 !important;
    }
    .main-content {
        padding-bottom: 100px; /* ê³ ì • ì…ë ¥ì°½ì„ ìœ„í•œ í•˜ë‹¨ ì—¬ë°± */
    }
    .debate-container {
        display: flex;
        justify-content: space-between;
        gap: 20px;
        margin-bottom: 15px;
    }
    .debate-column {
        flex: 1;
        max-width: 48%;
    }
    </style>
""", unsafe_allow_html=True)

# UI Layout
st.title("ğŸ¤– í† ë¡  ì±—ë´‡")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.subheader("ì„¤ì •")
    
    # ì°¬ì„± ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìˆ˜ì •
    st.text_area(
        "ì°¬ì„± ì±—ë´‡ ì„¤ì •", 
        value=st.session_state.system_message_pro,
        key="system_message_pro_input",
        height=150
    )
    
    # ë°˜ëŒ€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìˆ˜ì •
    st.text_area(
        "ë°˜ëŒ€ ì±—ë´‡ ì„¤ì •", 
        value=st.session_state.system_message_con,
        key="system_message_con_input",
        height=150
    )
    
    if st.button("ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸"):
        st.session_state.system_message_pro = st.session_state.system_message_pro_input
        st.session_state.system_message_con = st.session_state.system_message_con_input
        st.success("ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ê¸°íƒ€ ì‚¬ì´ë“œë°” ìš”ì†Œ
    st.markdown("---")
    
    # ì±„íŒ… ê¸°ë¡ ë³´ê¸°
    with st.expander("ì±„íŒ… ê¸°ë¡ ë³´ê¸°"):
        st.json(st.session_state.messages)
    
    # ì‚¬ìš©ëŸ‰ í†µê³„ ë³´ê¸°
    with st.expander("ì‚¬ìš©ëŸ‰ í†µê³„ ë³´ê¸°"):
        if st.session_state.usage_stats:
            for i, usage in enumerate(st.session_state.usage_stats):
                st.write(f"ë©”ì‹œì§€ {i+1}:")
                st.write(f"- í”„ë¡¬í”„íŠ¸ í† í°: {usage['prompt_tokens']}")
                st.write(f"- ì‘ë‹µ í† í°: {usage['completion_tokens']}")
                st.write(f"- ì´ í† í°: {usage['total_tokens']}")
                st.divider()
            
            # ì´ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            total_prompt = sum(u["prompt_tokens"] for u in st.session_state.usage_stats)
            total_completion = sum(u["completion_tokens"] for u in st.session_state.usage_stats)
            total = sum(u["total_tokens"] for u in st.session_state.usage_stats)
            
            st.write("### ì´ ì‚¬ìš©ëŸ‰")
            st.write(f"- ì´ í”„ë¡¬í”„íŠ¸ í† í°: {total_prompt}")
            st.write(f"- ì´ ì‘ë‹µ í† í°: {total_completion}")
            st.write(f"- ì´ í† í°: {total}")
        else:
            st.write("ì•„ì§ ì‚¬ìš©ëŸ‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì±„íŒ… ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ì±„íŒ… ì´ˆê¸°í™”"):
        st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! í† ë¡  ì±„íŒ…ë°©ì…ë‹ˆë‹¤. í† ë¡ í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "type": "system"}]
        st.session_state.usage_stats = []
        st.success("ì±„íŒ… ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # í”„ë¡œì„¸ìŠ¤ í‘œì‹œ í† ê¸€
    st.markdown("---")
    st.session_state.show_process = st.checkbox("ëª¨ë¸ ì²˜ë¦¬ ê³¼ì • ë³´ê¸°", value=st.session_state.show_process)
    
# ë©”ì¸ ì±„íŒ… ì˜ì—­
chat_container = st.container()
with chat_container:
    st.markdown('<div class="main-content">', unsafe_allow_html=True)
    
    # ì±„íŒ… ê¸°ë¡ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•˜ë©´ì„œ ì‚¬ìš©ìì™€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ì „ì²´ ë„ˆë¹„ë¡œ, ì°¬ì„±/ë°˜ëŒ€ëŠ” 2ì—´ë¡œ í‘œì‹œ
    i = 0
    while i < len(st.session_state.messages):
        message = st.session_state.messages[i]
        
        if message["role"] == "user":
            # ì‚¬ìš©ì ë©”ì‹œì§€ - ì „ì²´ ë„ˆë¹„ë¡œ í‘œì‹œ
            st.markdown(
                f"<div class='user-bubble'>{message['content']}</div>",
                unsafe_allow_html=True
            )
            i += 1
        elif message["role"] == "assistant" and "type" in message and message["type"] == "system":
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ - ì „ì²´ ë„ˆë¹„ë¡œ í‘œì‹œ
            st.markdown(
                f"<div class='system-bubble'>{message['content']}</div>",
                unsafe_allow_html=True
            )
            i += 1
        elif message["role"] == "assistant" and "type" in message and message["type"] == "pro":
            # ì°¬ì„± ë©”ì‹œì§€ê°€ ìˆìœ¼ë©´ ë‹¤ìŒ ë©”ì‹œì§€ê°€ ë°˜ëŒ€ ë©”ì‹œì§€ì¸ì§€ í™•ì¸
            pro_content = message["content"]
            con_content = ""
            
            # ë‹¤ìŒ ë©”ì‹œì§€ê°€ ë°˜ëŒ€ ë©”ì‹œì§€ì¸ì§€ í™•ì¸
            if i + 1 < len(st.session_state.messages) and st.session_state.messages[i + 1]["role"] == "assistant" and "type" in st.session_state.messages[i + 1] and st.session_state.messages[i + 1]["type"] == "con":
                con_content = st.session_state.messages[i + 1]["content"]
                i += 2  # ë‘ ë©”ì‹œì§€ë¥¼ ëª¨ë‘ ì²˜ë¦¬í–ˆìœ¼ë¯€ë¡œ ì¸ë±ìŠ¤ë¥¼ 2 ì¦ê°€
            else:
                i += 1  # ë°˜ëŒ€ ë©”ì‹œì§€ê°€ ì—†ìœ¼ë©´ ì¸ë±ìŠ¤ë¥¼ 1ë§Œ ì¦ê°€
            
            # 2ì—´ ë ˆì´ì•„ì›ƒ ìƒì„± (ë” ë„“ì€ ê°„ê²©ìœ¼ë¡œ ì¡°ì •)
            cols = st.columns([1, 0.2, 1])  # ì™¼ìª½ ì¹¼ëŸ¼, ê°„ê²©, ì˜¤ë¥¸ìª½ ì¹¼ëŸ¼
            
            # ì°¬ì„± ì˜ê²¬ (ì™¼ìª½ ì¹¼ëŸ¼)
            with cols[0]:
                st.markdown(
                    f"<div class='pro-bubble'>{pro_content}</div>",
                    unsafe_allow_html=True
                )
            
            # ë°˜ëŒ€ ì˜ê²¬ì´ ìˆìœ¼ë©´ ì˜¤ë¥¸ìª½ ì¹¼ëŸ¼ì— í‘œì‹œ
            if con_content:
                with cols[2]:
                    st.markdown(
                        f"<div class='con-bubble'>{con_content}</div>",
                        unsafe_allow_html=True
                    )
        elif message["role"] == "assistant" and "type" in message and message["type"] == "con":
            # ì´ë¯¸ ì²˜ë¦¬ëœ ê²½ìš° ìŠ¤í‚µ (ìœ„ì˜ pro ì²˜ë¦¬ ê³¼ì •ì—ì„œ í•¨ê»˜ ì²˜ë¦¬ë¨)
            # ë…ë¦½ì ì¸ con ë©”ì‹œì§€ë§Œ ì²˜ë¦¬
            cols = st.columns([1, 0.2, 1])
            with cols[2]:
                st.markdown(
                    f"<div class='con-bubble'>{message['content']}</div>",
                    unsafe_allow_html=True
                )
            i += 1
        else:
            # ê¸°íƒ€ ë©”ì‹œì§€
            st.markdown(
                f"<div class='system-bubble'>{message['content']}</div>",
                unsafe_allow_html=True
            )
            i += 1
    
    st.markdown('</div>', unsafe_allow_html=True)

# ì±„íŒ… ì…ë ¥
if prompt := st.chat_input("í† ë¡ í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
    st.markdown(
        f"<div class='user-bubble'>{prompt}</div>",
        unsafe_allow_html=True
    )
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê¸°ë¡ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ì°¬ì„±/ë°˜ëŒ€ ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    generate_debate_responses(prompt)