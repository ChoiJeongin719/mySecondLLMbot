import os
import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv
import json
import glob
import datetime  # ì‹œê°„ ì¸¡ì •ì„ ìœ„í•´ ì¶”ê°€

# Load environment variables from .env file
load_dotenv()

# Set page config
st.set_page_config(
    page_title="í† ë¡  ì±—ë´‡",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Initialize session state variables if they don't exist
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! í† ë¡  ì±„íŒ…ë°©ì…ë‹ˆë‹¤. í† ë¡ í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "type": "system"}]

if "system_message_pro" not in st.session_state:
    st.session_state.system_message_pro = "ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì œì‹œí•œ ì£¼ì œì— ëŒ€í•´ ì°¬ì„± ì…ì¥ì„ ì·¨í•˜ëŠ” í† ë¡ ìì…ë‹ˆë‹¤. ë…¼ë¦¬ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ì°¬ì„± ì…ì¥ì˜ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”. 4ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë°˜ë§ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."

if "system_message_con" not in st.session_state:
    st.session_state.system_message_con = "ë‹¹ì‹ ì€ ì‚¬ìš©ìê°€ ì œì‹œí•œ ì£¼ì œì— ëŒ€í•´ ë°˜ëŒ€ ì…ì¥ì„ ì·¨í•˜ëŠ” í† ë¡ ìì…ë‹ˆë‹¤. ë…¼ë¦¬ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ë°˜ëŒ€ ì…ì¥ì˜ ì˜ê²¬ì„ ì œì‹œí•´ì£¼ì„¸ìš”. 4ë¬¸ì¥ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ë°˜ë§ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."

if "usage_stats" not in st.session_state:
    st.session_state.usage_stats = []

if "show_process" not in st.session_state:
    st.session_state.show_process = False

# ì„¸ì…˜ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ ë³€ìˆ˜ë“¤ ì¶”ê°€
if "session_start_time" not in st.session_state:
    st.session_state.session_start_time = datetime.datetime.now()

if "last_interaction_time" not in st.session_state:
    st.session_state.last_interaction_time = datetime.datetime.now()

if "total_session_duration" not in st.session_state:
    st.session_state.total_session_duration = datetime.timedelta(0)

if "interaction_count" not in st.session_state:
    st.session_state.interaction_count = 0

# ì‚¬ìš© ì‹œê°„ ê¸°ë¡ í•¨ìˆ˜
def update_session_time():
    """í˜„ì¬ ì„¸ì…˜ì˜ ì‚¬ìš© ì‹œê°„ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤"""
    now = datetime.datetime.now()
    
    # ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì´í›„ 10ë¶„(600ì´ˆ) ì´ìƒ ì§€ë‚¬ë‹¤ë©´ ìƒˆ ì„¸ì…˜ìœ¼ë¡œ ê°„ì£¼
    time_diff = (now - st.session_state.last_interaction_time).total_seconds()
    if time_diff > 600:  # 10ë¶„ ì´ìƒ ì°¨ì´
        # ì´ì „ ì„¸ì…˜ ì‹œê°„ ì €ì¥
        session_duration = st.session_state.last_interaction_time - st.session_state.session_start_time
        st.session_state.total_session_duration += session_duration
        
        # ìƒˆ ì„¸ì…˜ ì‹œì‘
        st.session_state.session_start_time = now
    
    # ë§ˆì§€ë§‰ ìƒí˜¸ì‘ìš© ì‹œê°„ ì—…ë°ì´íŠ¸
    st.session_state.last_interaction_time = now
    
    # ìƒí˜¸ì‘ìš© íšŸìˆ˜ ì¦ê°€
    st.session_state.interaction_count += 1

def get_openai_client():
    """Create and return an OpenAI client configured with environment variables"""
    token = os.getenv("GITHUB_TOKEN")
    endpoint = os.getenv("GITHUB_ENDPOINT", "https://models.github.ai/inference")
    
    if not token:
        st.error("GitHub token not found in environment variables. Please check your .env file.")
        st.stop()
        
    return OpenAI(
        base_url=endpoint,
        api_key=token,
    )

def generate_debate_responses(prompt):
    """Generate two separate responses - one pro, one con"""
    # ìƒí˜¸ì‘ìš© ì‹œê°„ ì—…ë°ì´íŠ¸
    update_session_time()
    
    client = get_openai_client()
    model_name = os.getenv("GITHUB_MODEL", "openai/gpt-4o")
    
    # Prepare previous conversation history (excluding the system message)
    history = []
    for msg in st.session_state.messages:
        if msg["role"] != "system" and "type" not in msg:  # Skip system messages
            history.append(msg)
    
    # ì°¬ì„± ë©”ì‹œì§€ ìƒì„±
    pro_messages = [{"role": "system", "content": st.session_state.system_message_pro}] + history + [{"role": "user", "content": prompt}]
    
    # ë°˜ëŒ€ ë©”ì‹œì§€ ìƒì„±
    con_messages = [{"role": "system", "content": st.session_state.system_message_con}] + history + [{"role": "user", "content": prompt}]
    
    try:
        # ì±„íŒ…ë°© í˜•ì‹ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì‘ë‹µ í‘œì‹œ
        status_placeholder = st.empty()
        status_placeholder.markdown("ì‘ë‹µ ìƒì„± ì¤‘...", unsafe_allow_html=True)
        
        # ì°¬ì„± ì‘ë‹µ ìƒì„±
        full_pro_response = ""
        usage_pro = None
        
        pro_response = client.chat.completions.create(
            messages=pro_messages,
            model=model_name,
            stream=True,
            stream_options={'include_usage': True}
        )
        
        # ì°¬ì„± ì‘ë‹µ ì¶”ê°€
        pro_placeholder = st.empty()
        
        # ì°¬ì„± ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        for chunk in pro_response:
            if chunk.choices and chunk.choices[0].delta.content:
                content_chunk = chunk.choices[0].delta.content
                full_pro_response += content_chunk
                pro_placeholder.markdown(
                    f"<div class='pro-name'>ë³´ë¼</div><div class='pro-bubble'>{full_pro_response}â–Œ</div>",
                    unsafe_allow_html=True
                )
                
            if chunk.usage:
                usage_pro = chunk.usage
        
        # ìµœì¢… ì°¬ì„± ì‘ë‹µ ì—…ë°ì´íŠ¸ (ì»¤ì„œ ì œê±°)
        pro_placeholder.markdown(
            f"<div class='pro-name'>ë³´ë¼</div><div class='pro-bubble'>{full_pro_response}</div>",
            unsafe_allow_html=True
        )
        
        # ë°˜ëŒ€ ì‘ë‹µ ìƒì„±
        full_con_response = ""
        usage_con = None
        
        con_response = client.chat.completions.create(
            messages=con_messages,
            model=model_name,
            stream=True,
            stream_options={'include_usage': True}
        )
        
        # ë°˜ëŒ€ ì‘ë‹µ ì¶”ê°€
        con_placeholder = st.empty()
        
        # ë°˜ëŒ€ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
        for chunk in con_response:
            if chunk.choices and chunk.choices[0].delta.content:
                content_chunk = chunk.choices[0].delta.content
                full_con_response += content_chunk
                con_placeholder.markdown(
                    f"<div class='con-name'>ë…¸ë‘ì´</div><div class='con-bubble'>{full_con_response}â–Œ</div>",
                    unsafe_allow_html=True
                )
                
            if chunk.usage:
                usage_con = chunk.usage
        
        # ìµœì¢… ë°˜ëŒ€ ì‘ë‹µ ì—…ë°ì´íŠ¸ (ì»¤ì„œ ì œê±°)
        con_placeholder.markdown(
            f"<div class='con-name'>ë…¸ë‘ì´</div><div class='con-bubble'>{full_con_response}</div>",
            unsafe_allow_html=True
        )
        
        # ìƒíƒœ í‘œì‹œ ì œê±°
        status_placeholder.empty()
        
        # ì‘ë‹µì„ ì„¸ì…˜ ìƒíƒœì— ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": full_pro_response, "type": "pro"})
        st.session_state.messages.append({"role": "assistant", "content": full_con_response, "type": "con"})
        
        # ì‚¬ìš©ëŸ‰ í†µê³„ ì €ì¥
        if usage_pro and usage_con:
            # Pydantic ì²˜ë¦¬
            usage_pro_dict = usage_pro.model_dump() if hasattr(usage_pro, 'model_dump') else usage_pro.dict()
            usage_con_dict = usage_con.model_dump() if hasattr(usage_con, 'model_dump') else usage_con.dict()
            
            st.session_state.usage_stats.append({
                "prompt_tokens": usage_pro_dict.get("prompt_tokens", 0) + usage_con_dict.get("prompt_tokens", 0),
                "completion_tokens": usage_pro_dict.get("completion_tokens", 0) + usage_con_dict.get("completion_tokens", 0),
                "total_tokens": usage_pro_dict.get("total_tokens", 0) + usage_con_dict.get("total_tokens", 0)
            })
        
        # í”„ë¡œì„¸ìŠ¤ í‘œì‹œ í™œì„±í™”ëœ ê²½ìš°
        if st.session_state.show_process:
            process_container = st.container()
            with process_container:
                st.markdown("### ëª¨ë¸ ì²˜ë¦¬ ê³¼ì •")
                
                # ìš”ì²­ ìƒì„¸ ì •ë³´ í‘œì‹œ
                request_expander = st.expander("ìš”ì²­ ìƒì„¸ ì •ë³´", expanded=False)
                with request_expander:
                    st.markdown("**ì°¬ì„± ì‹œìŠ¤í…œ ë©”ì‹œì§€:**")
                    st.code(st.session_state.system_message_pro)
                    st.markdown("**ë°˜ëŒ€ ì‹œìŠ¤í…œ ë©”ì‹œì§€:**")
                    st.code(st.session_state.system_message_con)
                    st.markdown("**ì‚¬ìš©ì ì…ë ¥:**")
                    st.code(prompt)
                
                # ì›ì‹œ ì‘ë‹µ í‘œì‹œ
                response_expander = st.expander("ì›ì‹œ ì‘ë‹µ", expanded=False)
                with response_expander:
                    st.markdown("**ì°¬ì„± ì‘ë‹µ:**")
                    st.code(full_pro_response, language="markdown")
                    st.markdown("**ë°˜ëŒ€ ì‘ë‹µ:**")
                    st.code(full_con_response, language="markdown")
                
                # ì‚¬ìš©ëŸ‰ í†µê³„ í‘œì‹œ
                if usage_pro and usage_con:
                    usage_expander = st.expander("ì‚¬ìš©ëŸ‰ í†µê³„", expanded=False)
                    with usage_expander:
                        st.markdown("**ì°¬ì„± ì‘ë‹µ ì‚¬ìš©ëŸ‰:**")
                        st.markdown(f"- í”„ë¡¬í”„íŠ¸ í† í°: {usage_pro_dict.get('prompt_tokens', 0)}")
                        st.markdown(f"- ì‘ë‹µ í† í°: {usage_pro_dict.get('completion_tokens', 0)}")
                        st.markdown(f"- ì´ í† í°: {usage_pro_dict.get('total_tokens', 0)}")
                        
                        st.markdown("**ë°˜ëŒ€ ì‘ë‹µ ì‚¬ìš©ëŸ‰:**")
                        st.markdown(f"- í”„ë¡¬í”„íŠ¸ í† í°: {usage_con_dict.get('prompt_tokens', 0)}")
                        st.markdown(f"- ì‘ë‹µ í† í°: {usage_con_dict.get('completion_tokens', 0)}")
                        st.markdown(f"- ì´ í† í°: {usage_con_dict.get('total_tokens', 0)}")
        
        return True
    except Exception as e:
        st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

# CSS ìŠ¤íƒ€ì¼ ì •ì˜ ìˆ˜ì •
st.markdown("""
    <style>
    .pro-bubble {
        background-color: #f1f1f1;
        padding: 12px;
        border-radius: 18px 18px 18px 0;
        margin-bottom: 16px;
        max-width: 68%;  /* ìˆ˜ì •ëœ í¬ê¸° ìœ ì§€ */
        position: relative;
        margin-left: 50px;
        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
    }

    .pro-bubble::before {
        content: "";
        width: 40px;
        height: 40px;
        border-radius: 50%;
        background-color: #9370DB;  /* ë³´ë¼ìƒ‰ */
        position: absolute;
        left: -50px;
        top: 0;
        background-image: url('https://cdn-icons-png.flaticon.com/512/4712/4712035.png');
        background-size: 60%;
        background-position: center;
        background-repeat: no-repeat;
        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
    }

    .con-bubble {
        background-color: #f1f1f1;
        padding: 12px;
        border-radius: 18px 18px 18px 0;
        margin-bottom: 16px;
        max-width: 68%;  /* ë™ì¼í•˜ê²Œ 68%ë¡œ ë§ì¶¤ */
        position: relative;
        margin-left: 50px;
        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
    }

    .con-bubble::before {
        content: "";
        width: 40px;
        height: 40px;
        border-radius: 50%;
        background-color: #FFD700;  /* ë…¸ë‘ìƒ‰ */
        position: absolute;
        left: -50px;
        top: 0;
        background-image: url('https://cdn-icons-png.flaticon.com/512/4712/4712035.png');
        background-size: 60%;
        background-position: center;
        background-repeat: no-repeat;
        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
    }

    .user-bubble {
        background-color: #e6f2ff;
        padding: 12px;
        border-radius: 18px 18px 0 18px;
        margin-bottom: 16px;
        text-align: left;
        max-width: 60%;
        margin-left: auto;
        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
    }

    .system-bubble {
        background-color: #f8f9fa;
        padding: 12px;
        border-radius: 18px;
        margin-bottom: 16px;
        max-width: 70%;
        margin-left: auto;
        margin-right: auto;
        text-align: center;
        box-shadow: 0 1px 2px rgba(0,0,0,0.1);
    }

    .stChatFloatingInputContainer {
        position: fixed !important;
        bottom: 0 !important;
        padding: 1rem !important;
        width: calc(100% - 250px) !important; /* ì‚¬ì´ë“œë°” ë„ˆë¹„ ì¡°ì • */
        background-color: white !important;
        z-index: 1000 !important;
    }

    .main-content {
        padding-bottom: 100px; /* ê³ ì • ì…ë ¥ì°½ì„ ìœ„í•œ í•˜ë‹¨ ì—¬ë°± */
    }

    .pro-name {
        font-size: 0.8em;
        color: #9370DB;  /* ë³´ë¼ìƒ‰ */
        font-weight: bold;
        margin-bottom: 4px;
        margin-left: 0px;
    }

    .con-name {
        font-size: 0.8em;
        color: #FFD700;  /* ë…¸ë‘ìƒ‰ìœ¼ë¡œ ë³€ê²½ */
        font-weight: bold;
        margin-bottom: 4px;
        margin-left: 0px;
    }
    </style>
""", unsafe_allow_html=True)

# UI Layout
st.title("ğŸ¤– í† ë¡  ì±—ë´‡")

# ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.subheader("ì„¤ì •")
    
    # ì‹œê°„ ì¸¡ì • í†µê³„ í‘œì‹œ
    with st.expander("ì‚¬ìš© ì‹œê°„ í†µê³„", expanded=False):
        # í˜„ì¬ ì„¸ì…˜ ê³„ì‚°
        current_session_duration = st.session_state.last_interaction_time - st.session_state.session_start_time
        total_time = st.session_state.total_session_duration + current_session_duration
        
        # ì‹œê°„ í˜•ì‹ ì§€ì • (ì‹œ:ë¶„:ì´ˆ)
        def format_timedelta(td):
            total_seconds = int(td.total_seconds())
            hours = total_seconds // 3600
            minutes = (total_seconds % 3600) // 60
            seconds = total_seconds % 60
            return f"{hours:02d}:{minutes:02d}:{seconds:02d}"
        
        st.write("### ì‚¬ìš© ì‹œê°„")
        st.write(f"í˜„ì¬ ì„¸ì…˜: {format_timedelta(current_session_duration)}")
        st.write(f"ì´ ì‚¬ìš© ì‹œê°„: {format_timedelta(total_time)}")
        st.write(f"ì´ ìƒí˜¸ì‘ìš© íšŸìˆ˜: {st.session_state.interaction_count}")
        st.write(f"ì²« ì‚¬ìš© ì‹œê°„: {st.session_state.session_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        st.write(f"ë§ˆì§€ë§‰ ì‚¬ìš© ì‹œê°„: {st.session_state.last_interaction_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ì°¬ì„± ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìˆ˜ì •
    st.text_area(
        "ì°¬ì„± ì±—ë´‡ ì„¤ì •", 
        value=st.session_state.system_message_pro,
        key="system_message_pro_input",
        height=150
    )
    
    # ë°˜ëŒ€ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìˆ˜ì •
    st.text_area(
        "ë°˜ëŒ€ ì±—ë´‡ ì„¤ì •", 
        value=st.session_state.system_message_con,
        key="system_message_con_input",
        height=150
    )
    
    if st.button("ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì—…ë°ì´íŠ¸"):
        st.session_state.system_message_pro = st.session_state.system_message_pro_input
        st.session_state.system_message_con = st.session_state.system_message_con_input
        st.success("ì‹œìŠ¤í…œ ë©”ì‹œì§€ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # ê¸°íƒ€ ì‚¬ì´ë“œë°” ìš”ì†Œ
    st.markdown("---")
    
    # ì±„íŒ… ê¸°ë¡ ë³´ê¸°
    with st.expander("ì±„íŒ… ê¸°ë¡ ë³´ê¸°"):
        st.json(st.session_state.messages)
    
    # ì‚¬ìš©ëŸ‰ í†µê³„ ë³´ê¸°
    with st.expander("ì‚¬ìš©ëŸ‰ í†µê³„ ë³´ê¸°"):
        if st.session_state.usage_stats:
            for i, usage in enumerate(st.session_state.usage_stats):
                st.write(f"ë©”ì‹œì§€ {i+1}:")
                st.write(f"- í”„ë¡¬í”„íŠ¸ í† í°: {usage['prompt_tokens']}")
                st.write(f"- ì‘ë‹µ í† í°: {usage['completion_tokens']}")
                st.write(f"- ì´ í† í°: {usage['total_tokens']}")
                st.divider()
            
            # ì´ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            total_prompt = sum(u["prompt_tokens"] for u in st.session_state.usage_stats)
            total_completion = sum(u["completion_tokens"] for u in st.session_state.usage_stats)
            total = sum(u["total_tokens"] for u in st.session_state.usage_stats)
            
            st.write("### ì´ ì‚¬ìš©ëŸ‰")
            st.write(f"- ì´ í”„ë¡¬í”„íŠ¸ í† í°: {total_prompt}")
            st.write(f"- ì´ ì‘ë‹µ í† í°: {total_completion}")
            st.write(f"- ì´ í† í°: {total}")
        else:
            st.write("ì•„ì§ ì‚¬ìš©ëŸ‰ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ì±„íŒ… ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ì±„íŒ… ì´ˆê¸°í™”"):
        st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! í† ë¡  ì±„íŒ…ë°©ì…ë‹ˆë‹¤. í† ë¡ í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.", "type": "system"}]
        st.session_state.usage_stats = []
        
        # ì‹œê°„ ê´€ë ¨ ë³€ìˆ˜ë„ ì´ˆê¸°í™”
        now = datetime.datetime.now()
        st.session_state.session_start_time = now
        st.session_state.last_interaction_time = now
        st.session_state.total_session_duration = datetime.timedelta(0)
        st.session_state.interaction_count = 0
        
        st.success("ì±„íŒ… ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
    
    # í”„ë¡œì„¸ìŠ¤ í‘œì‹œ í† ê¸€
    st.markdown("---")
    st.session_state.show_process = st.checkbox("ëª¨ë¸ ì²˜ë¦¬ ê³¼ì • ë³´ê¸°", value=st.session_state.show_process)

# ë©”ì¸ ì±„íŒ… ì˜ì—­
chat_container = st.container()
with chat_container:
    st.markdown('<div class="main-content">', unsafe_allow_html=True)
    
    # ì±„íŒ… ê¸°ë¡ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
    i = 0
    while i < len(st.session_state.messages):
        message = st.session_state.messages[i]
        
        if message["role"] == "user":
            # ì‚¬ìš©ì ë©”ì‹œì§€
            st.markdown(
                f"<div class='user-bubble'>{message['content']}</div>",
                unsafe_allow_html=True
            )
            i += 1
        elif message["role"] == "assistant" and "type" in message and message["type"] == "system":
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€
            st.markdown(
                f"<div class='system-bubble'>{message['content']}</div>",
                unsafe_allow_html=True
            )
            i += 1
        elif message["role"] == "assistant" and "type" in message and message["type"] == "pro":
            # ì°¬ì„± ë©”ì‹œì§€ - ë³´ë¼ìƒ‰ í”„ë¡œí•„ë¡œ í‘œì‹œí•˜ê³  ì´ë¦„ì„ "ë³´ë¼"ë¡œ ë³€ê²½
            st.markdown(
                f"<div class='pro-name'>ë³´ë¼</div><div class='pro-bubble'>{message['content']}</div>",
                unsafe_allow_html=True
            )
            i += 1
        elif message["role"] == "assistant" and "type" in message and message["type"] == "con":
            # ë°˜ëŒ€ ë©”ì‹œì§€ - ë…¸ë‘ìƒ‰ í”„ë¡œí•„ë¡œ í‘œì‹œí•˜ê³  ì´ë¦„ì„ "ë…¸ë‘ì´"ë¡œ ë³€ê²½
            st.markdown(
                f"<div class='con-name'>ë…¸ë‘ì´</div><div class='con-bubble'>{message['content']}</div>",
                unsafe_allow_html=True
            )
            i += 1
        else:
            # ê¸°íƒ€ ë©”ì‹œì§€
            st.markdown(
                f"<div class='system-bubble'>{message['content']}</div>",
                unsafe_allow_html=True
            )
            i += 1
    
    st.markdown('</div>', unsafe_allow_html=True)

# ì±„íŒ… ì…ë ¥
if prompt := st.chat_input("í† ë¡ í•˜ê³  ì‹¶ì€ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”..."):
    # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ - ì±—ë´‡ ì•„ì´ì½˜ ì—†ì´ í‘œì‹œ
    st.markdown(
        f"<div class='user-bubble'>{prompt}</div>",
        unsafe_allow_html=True
    )
    
    # ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ê¸°ë¡ì— ì¶”ê°€
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # ì°¬ì„±/ë°˜ëŒ€ ì‘ë‹µ ìƒì„± ë° í‘œì‹œ
    generate_debate_responses(prompt)

